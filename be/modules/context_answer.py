import torch
from transformers import PreTrainedTokenizerFast, GPT2LMHeadModel
from rank_bm25 import BM25Okapi
import numpy as np

# Tokenizer ë° ëª¨ë¸ ë¡œë“œ
tokenizer = PreTrainedTokenizerFast.from_pretrained("skt/kogpt2-base-v2",
  bos_token='</s>', eos_token='</s>', unk_token='<unk>',
  pad_token='<pad>', mask_token='<mask>')
tokenizer.tokenize("ì•ˆë…•í•˜ì„¸ìš”. í•œêµ­ì–´ GPT-2 ì…ë‹ˆë‹¤.ğŸ˜¤:)l^o")
model = GPT2LMHeadModel.from_pretrained('skt/kogpt2-base-v2')

# BM25ìš© ìƒ˜í”Œ ë¬¸ì„œ ë° ì¿¼ë¦¬
documents = [
    "ê·¼ìœ¡ì´ ì»¤ì§€ê¸° ìœ„í•´ì„œëŠ” ìš´ë™ê³¼ ë‹¨ë°±ì§ˆ ì„­ì·¨ê°€ ì¤‘ìš”í•©ë‹ˆë‹¤.",
    "ìœ ì‚°ì†Œ ìš´ë™ê³¼ ê·¼ë ¥ ìš´ë™ì„ ë³‘í–‰í•˜ë©´ ì²´ì¤‘ ê°ì†Œì— ë„ì›€ì´ ë©ë‹ˆë‹¤.",
    "íƒ„ìˆ˜í™”ë¬¼ì€ ì—ë„ˆì§€ ê³µê¸‰ì›ìœ¼ë¡œ, ìš´ë™ ì „í›„ì— ì„­ì·¨í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤."
]
queries = ["í‚¤ê°€ í¬ë ¤ë©´?"]
# prompt = (
#     "ë‹¤ìŒ ì§ˆë¬¸ì— ëŒ€í•´ contextì˜ ë‚´ìš©ì„ ë°”íƒ•ìœ¼ë¡œ ëŒ€ë‹µí•´ì£¼ì„¸ìš”. \n\n"
#     "contextì— ì í•©í•œ ë‚´ìš©ì´ ì—†ìœ¼ë©´ 'ì‘ë‹µì¢…ë£Œ'ë¡œ ëŒ€ë‹µí•˜ì„¸ìš”. \n\n"
# )

# BM25 ì¸ë±ìŠ¤ êµ¬ì¶•
tokenized_docs = [doc.split() for doc in documents]
bm25 = BM25Okapi(tokenized_docs)

# ê²€ìƒ‰ ì¿¼ë¦¬ í† í¬ë‚˜ì´ì¦ˆ
query = queries[0]
tokenized_query = query.split()

# ì¿¼ë¦¬ë¡œë¶€í„° ê²€ìƒ‰ëœ ë¬¸ì„œì˜ ì¸ë±ìŠ¤ ê°€ì ¸ì˜¤ê¸°
scores = bm25.get_scores(tokenized_query)
best_doc_idx = np.argmax(scores)
retrieved_document = documents[best_doc_idx]

# KoGPT ëª¨ë¸ë¡œ í…ìŠ¤íŠ¸ ìƒì„±
# text = f"prompt: {prompt} \n\n context: {retrieved_document} \n\n query: {query}"
text = f"{retrieved_document} \n\n {query}"
input_ids = tokenizer.encode(text, return_tensors='pt')

# í…ìŠ¤íŠ¸ ìƒì„±
gen_ids = model.generate(
    input_ids,
    max_length=128,
    repetition_penalty=2.0,
    do_sample=True,
    temperature=0.3, # ìƒ˜í”Œë§ì˜ ì°½ì˜ì„± ì¡°ì •
    top_p=0.9,       # nucleus sampling
    pad_token_id=tokenizer.pad_token_id,
    eos_token_id=tokenizer.eos_token_id,
    bos_token_id=tokenizer.bos_token_id,
    use_cache=True
)

# ì‘ë‹µ ë””ì½”ë”© ë° ì¶œë ¥
generated = tokenizer.decode(gen_ids[0], skip_special_tokens=True)
print("Generated Response:", generated.split("."))
print("Generated Response:", generated.split(".")[0])
print("Generated Response:", generated.split(".")[1])
